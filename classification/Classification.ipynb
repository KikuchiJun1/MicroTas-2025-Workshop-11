{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ec0246",
   "metadata": {},
   "source": [
    "**Data layout (TIFF supported):**\n",
    "```\n",
    "./Data/\n",
    "  Train/\n",
    "    Normal/      *.tif, *.tiff, *.png, *.jpg\n",
    "    Abnormal/    *.tif, *.tiff, *.png, *.jpg\n",
    "  Validation/\n",
    "    Normal/      ...\n",
    "    Abnormal/    ...\n",
    "  Test/\n",
    "    Normal/      ...   (used for pretrained weight evaluation)\n",
    "    Abnormal/    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94fc8e",
   "metadata": {},
   "source": [
    "> **Colab/GitHub-ready:** This notebook uses relative paths (`./data`, `./checkpoints`) so it runs smoothly when opened from GitHub in Colab.  \n",
    "> **Training:** runs for **5 epochs** and saves to `./checkpoints/best_train.pth`.  \n",
    "> **Evaluation:** uses your pretrained file `./checkpoints/epoch=75-val_loss=0.69-val_acc=0.77.ckpt`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d637d",
   "metadata": {},
   "source": [
    "# ðŸ§ª Sperm Classification (Normal vs Abnormal) â€” DenseNet-169 (Colab Notebook)\n",
    "\n",
    "**Goal:** quick, accurate *trial* training run (10 epochs) using **DenseNetâ€‘169**. This notebook keeps key hyperparameters consistent with your setup: **image size 800Ã—800** and normalization `mean=0.2636`, `std=0.1562`. It also provides a simple evaluation section to load **your pretrained weights** and run on test images.\n",
    "\n",
    "> Tip: If you store data/weights in Google Drive, use the Drive mount cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcea56a",
   "metadata": {},
   "source": [
    "## 1) Runtime & Drive (optional)\n",
    "If your images/checkpoints live in Drive, mount it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Mount Google Drive if needed\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "#\n",
    "# After mounting, you can refer to files like:\n",
    "# data_dir = '/content/drive/MyDrive/your_dataset_root'\n",
    "# weights_path = '/content/drive/MyDrive/weights/densenet169_sperm.pth'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9493f0e1",
   "metadata": {},
   "source": [
    "## 2) Setup\n",
    "Colab usually has recent PyTorch/torchvision. If you need specific versions, uncomment the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f60501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you need to pin specific versions, uncomment and set the versions you prefer\n",
    "# !pip install --quiet torch torchvision torchmetrics==1.4.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c499294",
   "metadata": {},
   "source": [
    "## 3) Configuration\n",
    "Set your dataset paths, batch size, and training hyperparameters. We use **800Ã—800** images and the grayscale normalization stats you provided. We convert grayscale to **3 channels** inside the transform (to feed DenseNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ebe475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    # Paths\n",
    "    data_dir: str = \"./Data\"# root containing subfolders 'train' and 'val' (ImageFolder style)\n",
    "    # Example structure:\n",
    "    # /content/data/\n",
    "    #   train/\n",
    "    #     Normal/...\n",
    "    #     Abnormal/...\n",
    "    #   val/\n",
    "    #     Normal/...\n",
    "    #     Abnormal/...\n",
    "\n",
    "    # Training\n",
    "    epochs: int = 5# trial run as requested\n",
    "    batch_size: int = 8\n",
    "    num_workers: int = 2\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 0.0\n",
    "    seed: int = 42\n",
    "\n",
    "    # Image/Transforms â€” kept consistent with your code\n",
    "    img_size: int = 800\n",
    "    mean: float = 0.2636\n",
    "    std: float = 0.1562\n",
    "\n",
    "    # Checkpoints\n",
    "    save_dir: str = \"./checkpoints\"\n",
    "    ckpt_name: str = \"epoch=75-val_loss=0.69-val_acc=0.77.ckpt\"\n",
    "\n",
    "cfg = CFG()\n",
    "print(cfg)\n",
    "    train_ckpt_name: str = \"best_train.pth\"\n",
    "\n",
    "# Expect ImageFolder layout with /Train, /Validation, /Test under cfg.data_dir\n",
    "train_dir = os.path.join(cfg.data_dir, \"Train\")\n",
    "val_dir = os.path.join(cfg.data_dir, \"Validation\")\n",
    "test_dir = os.path.join(cfg.data_dir, \"Test\")\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
    "val_ds = datasets.ImageFolder(val_dir, transform=val_tfms)\n",
    "test_ds = datasets.ImageFolder(test_dir, transform=val_tfms)\n",
    "\n",
    "class_names = train_ds.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=cfg.num_workers, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2abceb4",
   "metadata": {},
   "source": [
    "## 4) Imports & Reproducibility\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c60285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, math, time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9097c9",
   "metadata": {},
   "source": [
    "## 5) Datasets & Dataloaders\n",
    "We keep your normalization and effective grayscale handling. We convert grayscale to 3 channels so DenseNetâ€‘169 can ingest it, but the **statistics remain the same** (we just repeat the single channel values across RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms â€” training includes flips; validation only resize+normalize.\n",
    "# We convert to 3 channels (RGB) so DenseNet works, but keep grayscale stats replicated across channels.\n",
    "\n",
    "IM_SIZE = cfg.img_size\n",
    "MEAN = [cfg.mean, cfg.mean, cfg.mean]\n",
    "STD = [cfg.std, cfg.std, cfg.std]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(MEAN, STD),\n",
    "])\n",
    "\n",
    "# Expect ImageFolder layout with /train and /val under cfg.data_dir\n",
    "train_dir = os.path.join(cfg.data_dir, \"train\")\n",
    "val_dir = os.path.join(cfg.data_dir, \"val\")\n",
    "\n",
    "train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
    "val_ds = datasets.ImageFolder(val_dir, transform=val_tfms)\n",
    "\n",
    "class_names = train_ds.classes\n",
    "num_classes = len(class_names)\n",
    "print(f\"Classes: {class_names} -> num_classes={num_classes}\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495666f",
   "metadata": {},
   "source": [
    "## 6) Model â€” DenseNetâ€‘169 (pretrained)\n",
    "We replace the classifier with a 2â€‘unit linear layer for binary classification. Loss: `CrossEntropyLoss`. Optimizer: `Adam` (lr=1e-4 by default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c413c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DenseNet-169\n",
    "dnet = models.densenet169(weights=models.DenseNet169_Weights.DEFAULT)\n",
    "\n",
    "# Replace classifier for our task\n",
    "in_features = dnet.classifier.in_features\n",
    "dnet.classifier = nn.Linear(in_features, 2)\n",
    "\n",
    "dnet = dnet.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(dnet.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e6d31",
   "metadata": {},
   "source": [
    "## 7) Training Loop (10 epochs)\n",
    "A minimal, wellâ€‘commented loop with val accuracy each epoch. Checkpoints are saved to `cfg.save_dir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89382fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "best_val_acc = 0.0\n",
    "history = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "def run_epoch(model, loader, train: bool) -> float:\n",
    "    epoch_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            outputs = model(images)           # logits\n",
    "            loss = criterion(outputs, labels) # CE loss\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # accuracy on the fly\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    avg_loss = epoch_loss / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "for epoch in range(1, cfg.epochs + 1):\n",
    "    t0 = time.time()\n",
    "    train_loss, _ = run_epoch(dnet, train_loader, train=True)\n",
    "    val_loss, val_acc = run_epoch(dnet, val_loader, train=False)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_path = os.path.join(cfg.save_dir, cfg.train_ckpt_name)\n",
    "        torch.save({\"model_state\": dnet.state_dict(),\n",
    "                    \"class_names\": class_names,\n",
    "                    \"cfg\": cfg.__dict__}, best_path)\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f\"Epoch {epoch:02d}/{cfg.epochs} | \"\n",
    "          f\"train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | val_acc: {val_acc:.4f} | \"\n",
    "          f\"time: {t1 - t0:.1f}s\")\n",
    "\n",
    "print(f\"Best val_acc: {best_val_acc:.4f} | Saved to: {os.path.join(cfg.save_dir, cfg.train_ckpt_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb45af23",
   "metadata": {},
   "source": [
    "## 8) Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca546a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss and val accuracy\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.title(\"Loss\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.legend(); plt.title(\"Validation Accuracy\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccdfb8e",
   "metadata": {},
   "source": [
    "## 9) Evaluation â€” Load Pretrained Weights and Test\n",
    "Point `weights_path` to your pretrained checkpoint (`.pth`) saved by this notebook **or your own weights**. We provide both **singleâ€‘image test** and **folderâ€‘based batch test** with a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c4d95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Evaluate PRETRAINED weights on TEST ===\n",
    "weights_path = os.path.join(cfg.save_dir, cfg.ckpt_name)\n",
    "model_inf, class_names_inf = load_model_for_inference(weights_path, num_classes=len(class_names))\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        logits = model_inf(images)\n",
    "        preds = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(labels.numpy().tolist())\n",
    "\n",
    "print(\"Classes:\", class_names_inf)\n",
    "print(classification_report(y_true, y_pred, target_names=test_ds.classes, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d745cd",
   "metadata": {},
   "source": [
    "### 9.a) Example: load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5129136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation will use the PRETRAINED checkpoint (do not overwrite).\n",
    "weights_path = os.path.join(cfg.save_dir, cfg.ckpt_name)\n",
    "model_inf, class_names_inf = load_model_for_inference(weights_path)\n",
    "print(\"Loaded inference model; classes:\", class_names_inf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4792871",
   "metadata": {},
   "source": [
    "### 9.b) Singleâ€‘image prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f779d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an example image\n",
    "# img_path = \"/content/data/val/Normal/example_001.png\"\n",
    "# result = predict_single_image(img_path, model_inf, class_names_inf)\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f2848",
   "metadata": {},
   "source": [
    "### 9.c) Batch evaluation on a folder (e.g., your validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb12f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_root = \"/content/data/val\"\n",
    "# cm, report, classes = evaluate_folder(data_root, model_inf, val_tfms)\n",
    "# print(\"Classes:\", classes)\n",
    "# print(\"Classification report:\\n\", report)\n",
    "\n",
    "# # Plot confusion matrix (no seaborn, simple matplotlib)\n",
    "# import itertools\n",
    "# plt.figure()\n",
    "# plt.imshow(cm, interpolation='nearest')\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.colorbar()\n",
    "# tick_marks = np.arange(len(classes))\n",
    "# plt.xticks(tick_marks, classes, rotation=45)\n",
    "# plt.yticks(tick_marks, classes)\n",
    "\n",
    "# thresh = cm.max() / 2.\n",
    "# for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#     plt.text(j, i, format(cm[i, j], 'd'),\n",
    "#              horizontalalignment=\"center\",\n",
    "#              color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "# plt.ylabel('True label')\n",
    "# plt.xlabel('Predicted label')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a34d4",
   "metadata": {},
   "source": [
    "## 10) Notes\n",
    "- Image size kept at **800Ã—800** to match your code.\n",
    "- Normalization kept at **mean=0.2636**, **std=0.1562** (replicated across 3 channels under the hood).\n",
    "- Random H/V flips used in training, none in validation â€” consistent with your approach.\n",
    "- You can easily swap the dataset paths or use Drive.\n",
    "- For a quick run, keep 10 epochs; adjust later as needed.\n",
    "- If your labels are reversed (0/1 mapping), just read `class_names` from `train_ds.classes` and interpret accordingly.\n",
    "- The evaluation helpers mimic your testing flow: softmax probs, perâ€‘image prediction, and a batch confusion matrix."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
