{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e22d2c97",
      "metadata": {
        "id": "e22d2c97"
      },
      "source": [
        "**Data layout (TIFF supported):**\n",
        "```\n",
        "./Data/\n",
        "  Train/\n",
        "    Normal/      *.tif, *.tiff, *.png, *.jpg\n",
        "    Abnormal/    *.tif, *.tiff, *.png, *.jpg\n",
        "  Validation/\n",
        "    Normal/\n",
        "    Abnormal/\n",
        "  Test/\n",
        "    Normal/      (used for pretrained-weight evaluation)\n",
        "    Abnormal/\n",
        "```\n",
        "Pretrained weights are auto-downloaded from Google Drive into `./checkpoints/<ckpt_name>` right before evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f94fc8e",
      "metadata": {
        "id": "4f94fc8e"
      },
      "source": [
        "> **Colab/GitHub-ready:** This notebook uses relative paths (`./data`, `./checkpoints`) so it runs smoothly when opened from GitHub in Colab.  \n",
        "> **Training:** runs for **5 epochs** and saves to `./checkpoints/best_train.pth`.  \n",
        "> **Evaluation:** uses your pretrained file `./checkpoints/epoch=75-val_loss=0.69-val_acc=0.77.ckpt`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9d637d",
      "metadata": {
        "id": "6d9d637d"
      },
      "source": [
        "# ðŸ§ª Sperm Classification (Normal vs Abnormal) â€” ResNet (Colab Notebook)\n",
        "\n",
        "**Goal:** quick, accurate *trial* training run (5 epochs) using **ResNet-34**. This notebook keeps key hyperparameters consistent with your setup: **image size 800Ã—800** and normalization `mean=0.2636`, `std=0.1562`. It also provides a simple evaluation section to load **your pretrained weights** and run on test images.\n",
        "\n",
        "> Tip: If you store data/weights in Google Drive, use the Drive mount cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fZqmUSqmQYtW"
      },
      "id": "fZqmUSqmQYtW"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Clone the repo only if needed; enter the correct folder ---\n",
        "import os, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/KikuchiJun1/MicroTas-2025-Workshop-11.git\"\n",
        "TARGET_DIR = Path(\"classification_repo\")\n",
        "\n",
        "def run(cmd, cwd=None):\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    r = subprocess.run(cmd, cwd=cwd, text=True,\n",
        "                       stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "    print(r.stdout)\n",
        "    r.check_returncode()\n",
        "    return r\n",
        "\n",
        "if not (TARGET_DIR / \".git\").exists():\n",
        "    if TARGET_DIR.exists() and any(TARGET_DIR.iterdir()):\n",
        "        raise RuntimeError(f\"Destination '{TARGET_DIR}' exists and is not empty.\")\n",
        "    print(\"Cloning repository...\")\n",
        "    run([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(TARGET_DIR)])\n",
        "else:\n",
        "    print(\"âœ“ Repo already cloned.\")\n",
        "    run([\"git\", \"fetch\", \"--all\", \"--prune\"], cwd=TARGET_DIR)\n",
        "    run([\"git\", \"pull\", \"--ff-only\"], cwd=TARGET_DIR)\n",
        "\n",
        "# Enter the 'classification' folder where Data/ lives\n",
        "os.chdir(TARGET_DIR)\n",
        "if Path(\"classification\").exists():\n",
        "    os.chdir(\"classification\")\n",
        "print(\"CWD:\", os.getcwd())\n",
        "print(\"Here:\", os.listdir(\".\")[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h5Fi7ZwJDZH",
        "outputId": "573b7208-7494-49b5-9669-786d2c4c8a91"
      },
      "id": "5h5Fi7ZwJDZH",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning repository...\n",
            "$ git clone --depth 1 https://github.com/KikuchiJun1/MicroTas-2025-Workshop-11.git classification_repo\n",
            "Cloning into 'classification_repo'...\n",
            "Updating files:  94% (3975/4212)\n",
            "Updating files:  95% (4002/4212)\n",
            "Updating files:  96% (4044/4212)\n",
            "Updating files:  97% (4086/4212)\n",
            "Updating files:  98% (4128/4212)\n",
            "Updating files:  99% (4170/4212)\n",
            "Updating files: 100% (4212/4212)\n",
            "Updating files: 100% (4212/4212), done.\n",
            "\n",
            "CWD: /content/classification_repo/classification/classification_repo/classification\n",
            "Here: ['Classification.ipynb', 'Classification_V3.ipynb', 'Data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0VYVrKlCQlsN"
      },
      "id": "0VYVrKlCQlsN"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Locate Data folder robustly & define CFG ---\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "# Find Data/\n",
        "CANDIDATES = [\n",
        "    Path(\"./Data\"),\n",
        "    Path(\"./classification/Data\"),\n",
        "    Path(\"./classification_repo/classification/Data\"),\n",
        "    Path(\"/content/classification_repo/classification/Data\"),\n",
        "]\n",
        "DATA_DIR = next((p.resolve() for p in CANDIDATES if p.exists()), None)\n",
        "if DATA_DIR is None:\n",
        "    raise FileNotFoundError(\"Couldn't find the Data folder. Checked:\\n\" + \"\\n\".join(map(str, CANDIDATES)))\n",
        "\n",
        "print(\"Using DATA_DIR:\", DATA_DIR)\n",
        "\n",
        "@dataclass\n",
        "class CFG:\n",
        "    # Paths\n",
        "    data_dir: str = str(DATA_DIR)\n",
        "    save_dir: str = \"./checkpoints\"\n",
        "    train_ckpt_name: str = \"resnet18_best.pth\"   # our training save name\n",
        "    # Training (keep as you had unless you change)\n",
        "    epochs: int = 5\n",
        "    batch_size: int = 8\n",
        "    num_workers: int = 0        # set >0 if your runtime is stable\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 0.0\n",
        "    seed: int = 42\n",
        "    # Image/Transforms (keep your workshop stats)\n",
        "    img_size: int = 800\n",
        "    mean: float = 0.2636\n",
        "    std: float = 0.1562\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "print(cfg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_nR6oyoJID3",
        "outputId": "177889d3-3ba0-40a4-a856-5814c26c0b87"
      },
      "id": "-_nR6oyoJID3",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using DATA_DIR: /content/classification_repo/classification/classification_repo/classification/Data\n",
            "CFG(data_dir='/content/classification_repo/classification/classification_repo/classification/Data', save_dir='./checkpoints', train_ckpt_name='resnet18_best.pth', epochs=5, batch_size=8, num_workers=0, lr=0.0001, weight_decay=0.0, seed=42, img_size=800, mean=0.2636, std=0.1562)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ec9nntASQpK-"
      },
      "id": "Ec9nntASQpK-"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Datasets, loaders, and (optional) training of ResNet-18 ---\n",
        "import random, numpy as np, torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch import nn, optim\n",
        "\n",
        "# Repro\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed); np.random.seed(seed)\n",
        "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True   # fixed size â†’ speed\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Transforms (grayscale -> 3ch; your stats & size)\n",
        "IM_SIZE = cfg.img_size\n",
        "MEAN = [cfg.mean]*3\n",
        "STD  = [cfg.std]*3\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "# Folders\n",
        "import os\n",
        "train_dir = os.path.join(cfg.data_dir, \"Train\")\n",
        "val_dir   = os.path.join(cfg.data_dir, \"Validation\")\n",
        "test_dir  = os.path.join(cfg.data_dir, \"Test\")\n",
        "\n",
        "# Datasets\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform=val_tfms)\n",
        "test_ds  = datasets.ImageFolder(test_dir,  transform=val_tfms)\n",
        "class_names = train_ds.classes\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "# Loaders\n",
        "pin = torch.cuda.is_available()\n",
        "loader_kwargs = dict(batch_size=cfg.batch_size, pin_memory=pin, num_workers=cfg.num_workers)\n",
        "train_loader = DataLoader(train_ds, shuffle=True,  **loader_kwargs)\n",
        "val_loader   = DataLoader(val_ds,   shuffle=False, **loader_kwargs)\n",
        "\n",
        "# --- Model: ALWAYS ResNet-18 ---\n",
        "resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "# Replace head\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
        "resnet = resnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "# Optional: quick training loop (run if you want to fine-tune; else skip this cell)\n",
        "def run_epoch(model, loader, train=True):\n",
        "    model.train(mode=train)\n",
        "    total, correct, loss_sum = 0, 0, 0.0\n",
        "    if train:\n",
        "        ctx = torch.enable_grad()\n",
        "    else:\n",
        "        ctx = torch.inference_mode()\n",
        "    with ctx:\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            if train: optimizer.zero_grad(set_to_none=True)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            if train:\n",
        "                loss.backward(); optimizer.step()\n",
        "            loss_sum += loss.item() * x.size(0)\n",
        "            pred = logits.argmax(1)\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "    return loss_sum / max(1,total), correct / max(1,total)\n",
        "\n",
        "best_val = 0.0\n",
        "for ep in range(1, cfg.epochs+1):\n",
        "    tr_loss, _ = run_epoch(resnet, train_loader, True)\n",
        "    va_loss, va_acc = run_epoch(resnet, val_loader, False)\n",
        "    if va_acc > best_val:\n",
        "        best_val = va_acc\n",
        "        torch.save({\"model_state\": resnet.state_dict(),\n",
        "                    \"class_names\": class_names}, os.path.join(cfg.save_dir, cfg.train_ckpt_name))\n",
        "    print(f\"Epoch {ep:02d}/{cfg.epochs} | train_loss {tr_loss:.4f} | val_loss {va_loss:.4f} | val_acc {va_acc:.4f}\")\n",
        "\n",
        "print(\"Best val_acc:\", best_val)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcRmaPz7JL49",
        "outputId": "79b38516-683f-4767-a035-9582ec56d578"
      },
      "id": "GcRmaPz7JL49",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Classes: ['Abnormal', 'Normal']\n",
            "Epoch 01/5 | train_loss 0.7080 | val_loss 1.3556 | val_acc 0.5000\n",
            "Epoch 02/5 | train_loss 0.7228 | val_loss 1.1421 | val_acc 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y91oKoFgQr3t"
      },
      "id": "Y91oKoFgQr3t"
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluate ResNet-18 on Data/Test with weights saved during training (resnet18_best.pth) ---\n",
        "import os, torch\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Preconditions\n",
        "assert 'cfg' in globals(), \"cfg not found. Run earlier chunks to define CFG.\"\n",
        "TRAIN_WEIGHTS_PATH = os.path.join(cfg.save_dir, cfg.train_ckpt_name)\n",
        "if not (os.path.exists(TRAIN_WEIGHTS_PATH) and os.path.getsize(TRAIN_WEIGHTS_PATH) > 0):\n",
        "    raise FileNotFoundError(\n",
        "        f\"Trained weights not found at {TRAIN_WEIGHTS_PATH}.\\n\"\n",
        "        f\"Run the training in Chunk 3 or set cfg.train_ckpt_name/cfg.save_dir accordingly.\"\n",
        "    )\n",
        "print(\"Using trained weights:\", TRAIN_WEIGHTS_PATH)\n",
        "\n",
        "# Locate Test\n",
        "DATA_DIR = Path(cfg.data_dir)\n",
        "TEST_DIR = DATA_DIR / \"Validation\"\n",
        "assert TEST_DIR.is_dir(), f\"Missing Test folder at: {TEST_DIR}\"\n",
        "print(\"Test dir:\", TEST_DIR)\n",
        "\n",
        "# Transforms (same stats/size as training)\n",
        "IM_SIZE = cfg.img_size\n",
        "MEAN = [cfg.mean]*3\n",
        "STD  = [cfg.std]*3\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "# Dataset/loader\n",
        "test_ds = datasets.ImageFolder(str(TEST_DIR), transform=val_tfms)\n",
        "class_names = test_ds.classes\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "pin = torch.cuda.is_available()\n",
        "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=0, pin_memory=pin)\n",
        "test_paths = [p for p, _ in test_ds.samples]\n",
        "\n",
        "# Model: ALWAYS ResNet-18\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_ckpt_any(path):\n",
        "    \"\"\"Safe loader for PyTorch 2.6+ (handles weights_only default).\"\"\"\n",
        "    try:\n",
        "        return torch.load(path, map_location=\"cpu\")  # tries weights_only=True first\n",
        "    except Exception:\n",
        "        try:\n",
        "            from torch.serialization import add_safe_globals\n",
        "            import torch.nn.functional as F\n",
        "            add_safe_globals([F.cross_entropy])\n",
        "            return torch.load(path, map_location=\"cpu\")\n",
        "        except Exception:\n",
        "            # Last resort (only if you trust the file)\n",
        "            return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "ckpt = load_ckpt_any(TRAIN_WEIGHTS_PATH)\n",
        "\n",
        "model_eval = models.resnet18(weights=None)\n",
        "model_eval.fc = nn.Linear(model_eval.fc.in_features, num_classes)\n",
        "\n",
        "state_dict = ckpt.get(\"model_state\", ckpt.get(\"state_dict\", ckpt))\n",
        "missing, unexpected = model_eval.load_state_dict(state_dict, strict=False)\n",
        "if missing or unexpected:\n",
        "    print(\"Note: non-strict load â†’ missing:\", missing, \"| unexpected:\", unexpected)\n",
        "\n",
        "model_eval = model_eval.to(device).eval()\n",
        "\n",
        "# Prefer saved class names if present\n",
        "class_names_inf = ckpt.get(\"class_names\", class_names)\n",
        "\n",
        "# Inference\n",
        "all_logits, all_labels = [], []\n",
        "with torch.inference_mode():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        logits = model_eval(x)\n",
        "        all_logits.append(logits.cpu())\n",
        "        all_labels.append(y)\n",
        "\n",
        "all_logits = torch.cat(all_logits, 0)\n",
        "all_labels = torch.cat(all_labels, 0).numpy()\n",
        "probs = torch.softmax(all_logits, dim=1).numpy()\n",
        "preds = probs.argmax(1)\n",
        "\n",
        "# Metrics\n",
        "acc = (preds == all_labels).mean()\n",
        "print(f\"\\nTest Accuracy (trained weights): {acc:.4f}\\n\")\n",
        "print(\"Classification report (trained weights):\")\n",
        "print(classification_report(all_labels, preds, target_names=class_names_inf, digits=4))\n",
        "\n",
        "# Confusion matrix (visual)\n",
        "cm = confusion_matrix(all_labels, preds)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title('Confusion Matrix (Test) â€” Trained Weights')\n",
        "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
        "    plt.xticks(range(len(class_names_inf)), class_names_inf, rotation=45, ha='right')\n",
        "    plt.yticks(range(len(class_names_inf)), class_names_inf)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, int(cm[i,j]), ha='center', va='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"(Skipping CM plot)\", e)\n",
        "\n",
        "# Per-image probabilities â†’ CSV (trained weights)\n",
        "import pandas as pd\n",
        "rows = []\n",
        "for i, path in enumerate(test_paths):\n",
        "    row = {\"path\": path,\n",
        "           \"true_label\": class_names_inf[all_labels[i]],\n",
        "           \"pred_label\": class_names_inf[preds[i]]}\n",
        "    for ci, cname in enumerate(class_names_inf):\n",
        "        row[f\"prob_{cname}\"] = float(probs[i, ci])\n",
        "    rows.append(row)\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"\\nPer-image probabilities (head) â€” trained weights:\")\n",
        "print(df.head(10))\n",
        "df.to_csv(\"test_predictions_trained.csv\", index=False)\n",
        "print(\"\\nSaved per-image predictions to: test_predictions_trained.csv\")\n"
      ],
      "metadata": {
        "id": "TW-Ee3VPL9dx"
      },
      "id": "TW-Ee3VPL9dx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ha55YBXTQvQl"
      },
      "id": "Ha55YBXTQvQl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Download pretrained ResNet-18 weights for evaluation (fixed URL) ---\n",
        "import os, sys, subprocess, shlex\n",
        "from pathlib import Path\n",
        "\n",
        "WEIGHTS_ID   = \"1pF4UOUmU9VU-mJU9MW6YWAINbCdKedMX\"  # << YOUR PROVIDED LINK\n",
        "WEIGHTS_URL  = f\"https://drive.google.com/uc?id={WEIGHTS_ID}\"\n",
        "WEIGHTS_PATH = os.path.join(\"checkpoints\", \"epoch=85-val_loss=0.97-val_acc=0.76.ckpt\")\n",
        "\n",
        "def _have(p):\n",
        "    return os.path.exists(p) and os.path.getsize(p) > 0\n",
        "\n",
        "os.makedirs(os.path.dirname(WEIGHTS_PATH), exist_ok=True)\n",
        "\n",
        "if not _have(WEIGHTS_PATH):\n",
        "    print(\"Attempting to download pretrained weights...\")\n",
        "    ok = False\n",
        "    try:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-U\", \"gdown>=5.2.0\"], check=True)\n",
        "        subprocess.run([\"gdown\", WEIGHTS_URL, \"-O\", WEIGHTS_PATH, \"--fuzzy\"], check=True)\n",
        "        ok = _have(WEIGHTS_PATH)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(\"gdown failed:\", e)\n",
        "\n",
        "    if not ok:\n",
        "        print(\"Trying wget fallback...\")\n",
        "        cookies = \"/tmp/gdcookies.txt\"\n",
        "        try:\n",
        "            cmd1 = f'wget --quiet --save-cookies {shlex.quote(cookies)} --keep-session-cookies --no-check-certificate \"{WEIGHTS_URL}\" -O-'\n",
        "            cmd2 = r\"sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' | head -n 1\"\n",
        "            token = subprocess.check_output(f\"{cmd1} | {cmd2}\", shell=True, text=True).strip()\n",
        "            if token:\n",
        "                dl = f\"https://drive.google.com/uc?export=download&confirm={token}&id={WEIGHTS_ID}\"\n",
        "                subprocess.run(f'wget --load-cookies {shlex.quote(cookies)} \"{dl}\" -O {shlex.quote(WEIGHTS_PATH)} -q --no-check-certificate', shell=True)\n",
        "        finally:\n",
        "            try: os.remove(cookies)\n",
        "            except OSError: pass\n",
        "        ok = _have(WEIGHTS_PATH)\n",
        "\n",
        "    if ok:\n",
        "        print(f\"âœ“ Downloaded to {WEIGHTS_PATH}\")\n",
        "    else:\n",
        "        raise RuntimeError(\"Could not download pretrained weights. Check Drive sharing or host elsewhere.\")\n",
        "else:\n",
        "    print(f\"âœ“ Weights already exist at {WEIGHTS_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dorDV0FzKvnl",
        "outputId": "d5ea2009-8f3e-4e2b-dbca-63852f868f6b"
      },
      "id": "dorDV0FzKvnl",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Weights already exist at checkpoints/epoch=85-val_loss=0.97-val_acc=0.76.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DpkLi1CJQwPV"
      },
      "id": "DpkLi1CJQwPV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Evaluate ResNet-18 on Data/Test with the downloaded pretrained weights ---\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Reuse config & stats\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Locate Test\n",
        "DATA_DIR = Path(cfg.data_dir)\n",
        "TEST_DIR = DATA_DIR / \"Validation\"\n",
        "assert TEST_DIR.is_dir(), f\"Missing Test folder at: {TEST_DIR}\"\n",
        "print(\"Test dir:\", TEST_DIR)\n",
        "\n",
        "# Transforms (keep same as training stats)\n",
        "IM_SIZE = cfg.img_size\n",
        "MEAN = [cfg.mean]*3\n",
        "STD  = [cfg.std]*3\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "# Dataset/loader\n",
        "test_ds = datasets.ImageFolder(str(TEST_DIR), transform=val_tfms)\n",
        "class_names = test_ds.classes\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes:\", class_names)\n",
        "\n",
        "test_loader = DataLoader(test_ds, batch_size=16, shuffle=False, num_workers=0, pin_memory=torch.cuda.is_available())\n",
        "test_paths = [p for p, _ in test_ds.samples]\n",
        "\n",
        "# --- Always build ResNet-18 and load weights safely (PyTorch 2.6+) ---\n",
        "def load_ckpt_any(path):\n",
        "    try:\n",
        "        return torch.load(path, map_location=\"cpu\")  # weights_only=True default in PT 2.6\n",
        "    except Exception:\n",
        "        try:\n",
        "            from torch.serialization import add_safe_globals\n",
        "            import torch.nn.functional as F\n",
        "            add_safe_globals([F.cross_entropy])\n",
        "            return torch.load(path, map_location=\"cpu\")\n",
        "        except Exception:\n",
        "            # Last resort if you trust the checkpoint\n",
        "            return torch.load(path, map_location=\"cpu\", weights_only=False)\n",
        "\n",
        "ckpt = load_ckpt_any(WEIGHTS_PATH)\n",
        "\n",
        "resnet_eval = models.resnet18(weights=None)  # head will be replaced; we load your trained weights\n",
        "resnet_eval.fc = nn.Linear(resnet_eval.fc.in_features, num_classes)\n",
        "missing, unexpected = resnet_eval.load_state_dict(\n",
        "    # Accept either full state_dict or under 'model_state'\n",
        "    ckpt.get(\"state_dict\", ckpt.get(\"model_state\", ckpt)),\n",
        "    strict=False\n",
        ")\n",
        "if missing or unexpected:\n",
        "    print(\"Note: non-strict load â†’ missing:\", missing, \"| unexpected:\", unexpected)\n",
        "\n",
        "resnet_eval = resnet_eval.to(device).eval()\n",
        "\n",
        "# If class names were saved inside the ckpt, prefer them for labeling\n",
        "class_names_inf = ckpt.get(\"class_names\", class_names)\n",
        "\n",
        "# Inference\n",
        "all_logits, all_labels = [], []\n",
        "with torch.inference_mode():\n",
        "    for x, y in test_loader:\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        logits = resnet_eval(x)\n",
        "        all_logits.append(logits.cpu())\n",
        "        all_labels.append(y)\n",
        "\n",
        "all_logits = torch.cat(all_logits, 0)\n",
        "all_labels = torch.cat(all_labels, 0).numpy()\n",
        "probs = torch.softmax(all_logits, dim=1).numpy()\n",
        "preds = probs.argmax(1)\n",
        "\n",
        "# Metrics\n",
        "acc = (preds == all_labels).mean()\n",
        "print(f\"\\nTest Accuracy: {acc:.4f}\\n\")\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(all_labels, preds, target_names=class_names_inf, digits=4))\n",
        "\n",
        "# Confusion matrix (visual)\n",
        "cm = confusion_matrix(all_labels, preds)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(cm, interpolation='nearest')\n",
        "    plt.title('Confusion Matrix (Test)')\n",
        "    plt.xlabel('Predicted'); plt.ylabel('True')\n",
        "    plt.xticks(range(len(class_names_inf)), class_names_inf, rotation=45, ha='right')\n",
        "    plt.yticks(range(len(class_names_inf)), class_names_inf)\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, int(cm[i,j]), ha='center', va='center')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(\"(Skipping CM plot)\", e)\n",
        "\n",
        "# Per-image probabilities â†’ CSV\n",
        "import pandas as pd\n",
        "rows = []\n",
        "for i, path in enumerate(test_paths):\n",
        "    row = {\"path\": path,\n",
        "           \"true_label\": class_names_inf[all_labels[i]],\n",
        "           \"pred_label\": class_names_inf[preds[i]]}\n",
        "    for ci, cname in enumerate(class_names_inf):\n",
        "        row[f\"prob_{cname}\"] = float(probs[i, ci])\n",
        "    rows.append(row)\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"\\nPer-image probabilities (head):\")\n",
        "print(df.head(10))\n",
        "df.to_csv(\"test_predictions.csv\", index=False)\n",
        "print(\"\\nSaved per-image predictions to: test_predictions.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r6ebhQPLLArU",
        "outputId": "680ed986-a392-4dd5-9d2c-af0cdd459b24"
      },
      "id": "r6ebhQPLLArU",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dir: /content/classification_repo/classification/Data/Validation\n",
            "Classes: ['Abnormal', 'Normal']\n",
            "Note: non-strict load â†’ missing: ['conv1.weight', 'bn1.weight', 'bn1.bias', 'bn1.running_mean', 'bn1.running_var', 'layer1.0.conv1.weight', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.conv2.weight', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.1.conv1.weight', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.conv2.weight', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer2.0.conv1.weight', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.conv2.weight', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.1.conv1.weight', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.conv2.weight', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer3.0.conv1.weight', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.conv2.weight', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.1.conv1.weight', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.conv2.weight', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer4.0.conv1.weight', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.conv2.weight', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.1.conv1.weight', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.conv2.weight', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'fc.weight', 'fc.bias'] | unexpected: ['model.conv1.weight', 'model.bn1.weight', 'model.bn1.bias', 'model.bn1.running_mean', 'model.bn1.running_var', 'model.bn1.num_batches_tracked', 'model.layer1.0.conv1.weight', 'model.layer1.0.bn1.weight', 'model.layer1.0.bn1.bias', 'model.layer1.0.bn1.running_mean', 'model.layer1.0.bn1.running_var', 'model.layer1.0.bn1.num_batches_tracked', 'model.layer1.0.conv2.weight', 'model.layer1.0.bn2.weight', 'model.layer1.0.bn2.bias', 'model.layer1.0.bn2.running_mean', 'model.layer1.0.bn2.running_var', 'model.layer1.0.bn2.num_batches_tracked', 'model.layer1.1.conv1.weight', 'model.layer1.1.bn1.weight', 'model.layer1.1.bn1.bias', 'model.layer1.1.bn1.running_mean', 'model.layer1.1.bn1.running_var', 'model.layer1.1.bn1.num_batches_tracked', 'model.layer1.1.conv2.weight', 'model.layer1.1.bn2.weight', 'model.layer1.1.bn2.bias', 'model.layer1.1.bn2.running_mean', 'model.layer1.1.bn2.running_var', 'model.layer1.1.bn2.num_batches_tracked', 'model.layer1.2.conv1.weight', 'model.layer1.2.bn1.weight', 'model.layer1.2.bn1.bias', 'model.layer1.2.bn1.running_mean', 'model.layer1.2.bn1.running_var', 'model.layer1.2.bn1.num_batches_tracked', 'model.layer1.2.conv2.weight', 'model.layer1.2.bn2.weight', 'model.layer1.2.bn2.bias', 'model.layer1.2.bn2.running_mean', 'model.layer1.2.bn2.running_var', 'model.layer1.2.bn2.num_batches_tracked', 'model.layer2.0.conv1.weight', 'model.layer2.0.bn1.weight', 'model.layer2.0.bn1.bias', 'model.layer2.0.bn1.running_mean', 'model.layer2.0.bn1.running_var', 'model.layer2.0.bn1.num_batches_tracked', 'model.layer2.0.conv2.weight', 'model.layer2.0.bn2.weight', 'model.layer2.0.bn2.bias', 'model.layer2.0.bn2.running_mean', 'model.layer2.0.bn2.running_var', 'model.layer2.0.bn2.num_batches_tracked', 'model.layer2.0.downsample.0.weight', 'model.layer2.0.downsample.1.weight', 'model.layer2.0.downsample.1.bias', 'model.layer2.0.downsample.1.running_mean', 'model.layer2.0.downsample.1.running_var', 'model.layer2.0.downsample.1.num_batches_tracked', 'model.layer2.1.conv1.weight', 'model.layer2.1.bn1.weight', 'model.layer2.1.bn1.bias', 'model.layer2.1.bn1.running_mean', 'model.layer2.1.bn1.running_var', 'model.layer2.1.bn1.num_batches_tracked', 'model.layer2.1.conv2.weight', 'model.layer2.1.bn2.weight', 'model.layer2.1.bn2.bias', 'model.layer2.1.bn2.running_mean', 'model.layer2.1.bn2.running_var', 'model.layer2.1.bn2.num_batches_tracked', 'model.layer2.2.conv1.weight', 'model.layer2.2.bn1.weight', 'model.layer2.2.bn1.bias', 'model.layer2.2.bn1.running_mean', 'model.layer2.2.bn1.running_var', 'model.layer2.2.bn1.num_batches_tracked', 'model.layer2.2.conv2.weight', 'model.layer2.2.bn2.weight', 'model.layer2.2.bn2.bias', 'model.layer2.2.bn2.running_mean', 'model.layer2.2.bn2.running_var', 'model.layer2.2.bn2.num_batches_tracked', 'model.layer2.3.conv1.weight', 'model.layer2.3.bn1.weight', 'model.layer2.3.bn1.bias', 'model.layer2.3.bn1.running_mean', 'model.layer2.3.bn1.running_var', 'model.layer2.3.bn1.num_batches_tracked', 'model.layer2.3.conv2.weight', 'model.layer2.3.bn2.weight', 'model.layer2.3.bn2.bias', 'model.layer2.3.bn2.running_mean', 'model.layer2.3.bn2.running_var', 'model.layer2.3.bn2.num_batches_tracked', 'model.layer3.0.conv1.weight', 'model.layer3.0.bn1.weight', 'model.layer3.0.bn1.bias', 'model.layer3.0.bn1.running_mean', 'model.layer3.0.bn1.running_var', 'model.layer3.0.bn1.num_batches_tracked', 'model.layer3.0.conv2.weight', 'model.layer3.0.bn2.weight', 'model.layer3.0.bn2.bias', 'model.layer3.0.bn2.running_mean', 'model.layer3.0.bn2.running_var', 'model.layer3.0.bn2.num_batches_tracked', 'model.layer3.0.downsample.0.weight', 'model.layer3.0.downsample.1.weight', 'model.layer3.0.downsample.1.bias', 'model.layer3.0.downsample.1.running_mean', 'model.layer3.0.downsample.1.running_var', 'model.layer3.0.downsample.1.num_batches_tracked', 'model.layer3.1.conv1.weight', 'model.layer3.1.bn1.weight', 'model.layer3.1.bn1.bias', 'model.layer3.1.bn1.running_mean', 'model.layer3.1.bn1.running_var', 'model.layer3.1.bn1.num_batches_tracked', 'model.layer3.1.conv2.weight', 'model.layer3.1.bn2.weight', 'model.layer3.1.bn2.bias', 'model.layer3.1.bn2.running_mean', 'model.layer3.1.bn2.running_var', 'model.layer3.1.bn2.num_batches_tracked', 'model.layer3.2.conv1.weight', 'model.layer3.2.bn1.weight', 'model.layer3.2.bn1.bias', 'model.layer3.2.bn1.running_mean', 'model.layer3.2.bn1.running_var', 'model.layer3.2.bn1.num_batches_tracked', 'model.layer3.2.conv2.weight', 'model.layer3.2.bn2.weight', 'model.layer3.2.bn2.bias', 'model.layer3.2.bn2.running_mean', 'model.layer3.2.bn2.running_var', 'model.layer3.2.bn2.num_batches_tracked', 'model.layer3.3.conv1.weight', 'model.layer3.3.bn1.weight', 'model.layer3.3.bn1.bias', 'model.layer3.3.bn1.running_mean', 'model.layer3.3.bn1.running_var', 'model.layer3.3.bn1.num_batches_tracked', 'model.layer3.3.conv2.weight', 'model.layer3.3.bn2.weight', 'model.layer3.3.bn2.bias', 'model.layer3.3.bn2.running_mean', 'model.layer3.3.bn2.running_var', 'model.layer3.3.bn2.num_batches_tracked', 'model.layer3.4.conv1.weight', 'model.layer3.4.bn1.weight', 'model.layer3.4.bn1.bias', 'model.layer3.4.bn1.running_mean', 'model.layer3.4.bn1.running_var', 'model.layer3.4.bn1.num_batches_tracked', 'model.layer3.4.conv2.weight', 'model.layer3.4.bn2.weight', 'model.layer3.4.bn2.bias', 'model.layer3.4.bn2.running_mean', 'model.layer3.4.bn2.running_var', 'model.layer3.4.bn2.num_batches_tracked', 'model.layer3.5.conv1.weight', 'model.layer3.5.bn1.weight', 'model.layer3.5.bn1.bias', 'model.layer3.5.bn1.running_mean', 'model.layer3.5.bn1.running_var', 'model.layer3.5.bn1.num_batches_tracked', 'model.layer3.5.conv2.weight', 'model.layer3.5.bn2.weight', 'model.layer3.5.bn2.bias', 'model.layer3.5.bn2.running_mean', 'model.layer3.5.bn2.running_var', 'model.layer3.5.bn2.num_batches_tracked', 'model.layer4.0.conv1.weight', 'model.layer4.0.bn1.weight', 'model.layer4.0.bn1.bias', 'model.layer4.0.bn1.running_mean', 'model.layer4.0.bn1.running_var', 'model.layer4.0.bn1.num_batches_tracked', 'model.layer4.0.conv2.weight', 'model.layer4.0.bn2.weight', 'model.layer4.0.bn2.bias', 'model.layer4.0.bn2.running_mean', 'model.layer4.0.bn2.running_var', 'model.layer4.0.bn2.num_batches_tracked', 'model.layer4.0.downsample.0.weight', 'model.layer4.0.downsample.1.weight', 'model.layer4.0.downsample.1.bias', 'model.layer4.0.downsample.1.running_mean', 'model.layer4.0.downsample.1.running_var', 'model.layer4.0.downsample.1.num_batches_tracked', 'model.layer4.1.conv1.weight', 'model.layer4.1.bn1.weight', 'model.layer4.1.bn1.bias', 'model.layer4.1.bn1.running_mean', 'model.layer4.1.bn1.running_var', 'model.layer4.1.bn1.num_batches_tracked', 'model.layer4.1.conv2.weight', 'model.layer4.1.bn2.weight', 'model.layer4.1.bn2.bias', 'model.layer4.1.bn2.running_mean', 'model.layer4.1.bn2.running_var', 'model.layer4.1.bn2.num_batches_tracked', 'model.layer4.2.conv1.weight', 'model.layer4.2.bn1.weight', 'model.layer4.2.bn1.bias', 'model.layer4.2.bn1.running_mean', 'model.layer4.2.bn1.running_var', 'model.layer4.2.bn1.num_batches_tracked', 'model.layer4.2.conv2.weight', 'model.layer4.2.bn2.weight', 'model.layer4.2.bn2.bias', 'model.layer4.2.bn2.running_mean', 'model.layer4.2.bn2.running_var', 'model.layer4.2.bn2.num_batches_tracked', 'model.fc.weight', 'model.fc.bias']\n",
            "\n",
            "Test Accuracy: 0.5000\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Abnormal     0.5000    1.0000    0.6667         4\n",
            "      Normal     0.0000    0.0000    0.0000         4\n",
            "\n",
            "    accuracy                         0.5000         8\n",
            "   macro avg     0.2500    0.5000    0.3333         8\n",
            "weighted avg     0.2500    0.5000    0.3333         8\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-image probabilities (head):\n",
            "                                                path true_label pred_label  \\\n",
            "0  /content/classification_repo/classification/Da...   Abnormal   Abnormal   \n",
            "1  /content/classification_repo/classification/Da...   Abnormal   Abnormal   \n",
            "2  /content/classification_repo/classification/Da...   Abnormal   Abnormal   \n",
            "3  /content/classification_repo/classification/Da...   Abnormal   Abnormal   \n",
            "4  /content/classification_repo/classification/Da...     Normal   Abnormal   \n",
            "5  /content/classification_repo/classification/Da...     Normal   Abnormal   \n",
            "6  /content/classification_repo/classification/Da...     Normal   Abnormal   \n",
            "7  /content/classification_repo/classification/Da...     Normal   Abnormal   \n",
            "\n",
            "   prob_Abnormal  prob_Normal  \n",
            "0       0.754535     0.245465  \n",
            "1       0.752382     0.247618  \n",
            "2       0.756716     0.243284  \n",
            "3       0.754375     0.245625  \n",
            "4       0.998577     0.001423  \n",
            "5       0.998577     0.001423  \n",
            "6       0.998577     0.001423  \n",
            "7       0.998577     0.001423  \n",
            "\n",
            "Saved per-image predictions to: test_predictions.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}