{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e22d2c97",
      "metadata": {
        "id": "e22d2c97"
      },
      "source": [
        "**Data layout (TIFF supported):**\n",
        "```\n",
        "./Data/\n",
        "  Train/\n",
        "    Normal/      *.tif, *.tiff, *.png, *.jpg\n",
        "    Abnormal/    *.tif, *.tiff, *.png, *.jpg\n",
        "  Validation/\n",
        "    Normal/\n",
        "    Abnormal/\n",
        "  Test/\n",
        "    Normal/      (used for pretrained-weight evaluation)\n",
        "    Abnormal/\n",
        "```\n",
        "Pretrained weights are auto-downloaded from Google Drive into `./checkpoints/<ckpt_name>` right before evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f94fc8e",
      "metadata": {
        "id": "4f94fc8e"
      },
      "source": [
        "> **Colab/GitHub-ready:** This notebook uses relative paths (`./data`, `./checkpoints`) so it runs smoothly when opened from GitHub in Colab.  \n",
        "> **Training:** runs for **5 epochs** and saves to `./checkpoints/best_train.pth`.  \n",
        "> **Evaluation:** uses your pretrained file `./checkpoints/epoch=75-val_loss=0.69-val_acc=0.77.ckpt`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9d637d",
      "metadata": {
        "id": "6d9d637d"
      },
      "source": [
        "# ðŸ§ª Sperm Classification (Normal vs Abnormal) â€” DenseNet-169 (Colab Notebook)\n",
        "\n",
        "**Goal:** quick, accurate *trial* training run (10 epochs) using **DenseNetâ€‘169**. This notebook keeps key hyperparameters consistent with your setup: **image size 800Ã—800** and normalization `mean=0.2636`, `std=0.1562`. It also provides a simple evaluation section to load **your pretrained weights** and run on test images.\n",
        "\n",
        "> Tip: If you store data/weights in Google Drive, use the Drive mount cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dcea56a",
      "metadata": {
        "id": "4dcea56a"
      },
      "source": [
        "## 1) Runtime & Drive (optional)\n",
        "If your images/checkpoints live in Drive, mount it first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448e5df9",
      "metadata": {
        "id": "448e5df9"
      },
      "outputs": [],
      "source": [
        "# (Optional) Mount Google Drive if needed\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "#\n",
        "# After mounting, you can refer to files like:\n",
        "# data_dir = '/content/drive/MyDrive/your_dataset_root'\n",
        "# weights_path = '/content/drive/MyDrive/weights/densenet169_sperm.pth'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9493f0e1",
      "metadata": {
        "id": "9493f0e1"
      },
      "source": [
        "## 2) Setup\n",
        "Colab usually has recent PyTorch/torchvision. If you need specific versions, uncomment the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "02f60501",
      "metadata": {
        "id": "02f60501",
        "outputId": "30f83576-4535-4384-8273-88e629a02035",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Repo already cloned. Pulling latest changes...\n",
            "$ git remote set-url origin https://github.com/KikuchiJun1/MicroTas-2025-Workshop-11.git\n",
            "\n",
            "$ git fetch --all --prune\n",
            "Fetching origin\n",
            "\n",
            "$ git pull --ff-only\n",
            "Already up to date.\n",
            "\n",
            "Working directory: /content/classification_repo\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "REPO_URL = \"https://github.com/KikuchiJun1/MicroTas-2025-Workshop-11.git\"\n",
        "TARGET_DIR = Path(\"classification_repo\")\n",
        "\n",
        "def run(cmd, cwd=None):\n",
        "    \"\"\"Run a shell command and print combined output; raise on failure.\"\"\"\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    r = subprocess.run(cmd, cwd=cwd, text=True,\n",
        "                       stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
        "    print(r.stdout)\n",
        "    r.check_returncode()\n",
        "    return r\n",
        "\n",
        "try:\n",
        "    if (TARGET_DIR / \".git\").exists():\n",
        "        print(\"âœ“ Repo already cloned. Pulling latest changes...\")\n",
        "        # Ensure the remote URL is correct, then pull\n",
        "        run([\"git\", \"remote\", \"set-url\", \"origin\", REPO_URL], cwd=TARGET_DIR)\n",
        "        run([\"git\", \"fetch\", \"--all\", \"--prune\"], cwd=TARGET_DIR)\n",
        "        run([\"git\", \"pull\", \"--ff-only\"], cwd=TARGET_DIR)\n",
        "    elif TARGET_DIR.exists() and any(TARGET_DIR.iterdir()):\n",
        "        # Folder exists but isn't a git repo -> avoid clone error 128.\n",
        "        raise RuntimeError(\n",
        "            f\"Destination path '{TARGET_DIR}' exists and is not an empty directory. \"\n",
        "            \"Either delete it or set TARGET_DIR to a new folder.\"\n",
        "        )\n",
        "    else:\n",
        "        print(\"Cloning repository...\")\n",
        "        run([\"git\", \"clone\", \"--depth\", \"1\", REPO_URL, str(TARGET_DIR)])\n",
        "        print(\"Repository cloned successfully!\")\n",
        "\n",
        "    # Move into the repo so later code can access its files\n",
        "    os.chdir(TARGET_DIR)\n",
        "    print(f\"Working directory: {os.getcwd()}\")\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(\"Git command failed with exit code\", e.returncode)\n",
        "    # The printed stdout above usually includes the error message, e.g.:\n",
        "    # - 'fatal: repository not found' (URL typo or private)\n",
        "    # - 'fatal: destination path ... already exists' (folder already there)\n",
        "    # - auth errors for private repos\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# robustly find the Data folder relative to wherever we are\n",
        "candidates = [\n",
        "    Path(\"./Data\"),\n",
        "    Path(\"./classification/Data\"),\n",
        "    Path(\"./classification_repo/classification/Data\"),\n",
        "]\n",
        "\n",
        "DATA_DIR = next((p.resolve() for p in candidates if p.exists()), None)\n",
        "if DATA_DIR is None:\n",
        "    raise FileNotFoundError(\"Couldn't find the Data folder. Checked: \" + \", \".join(map(str, candidates)))\n",
        "\n",
        "print(\"Using DATA_DIR:\", DATA_DIR)\n"
      ],
      "metadata": {
        "id": "BRbku6aR-SHd",
        "outputId": "423fbe92-6252-4180-d60b-bf99ba09fd16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BRbku6aR-SHd",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using DATA_DIR: /content/classification_repo/classification/Data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c499294",
      "metadata": {
        "id": "3c499294"
      },
      "source": [
        "## 3) Configuration\n",
        "Set your dataset paths, batch size, and training hyperparameters. We use **800Ã—800** images and the grayscale normalization stats you provided. We convert grayscale to **3 channels** inside the transform (to feed DenseNet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "40ebe475",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "40ebe475",
        "outputId": "6d72beb6-2b7a-410a-a486-a56bb09c5d98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CFG(data_dir='./Data', epochs=5, batch_size=8, num_workers=2, lr=0.0001, weight_decay=0.0, seed=42, img_size=800, mean=0.2636, std=0.1562, save_dir='./checkpoints', ckpt_name='epoch=75-val_loss=0.69-val_acc=0.77.ckpt', train_ckpt_name='best_train.pth')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './Data/Train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-974837537.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# ImageFolder will read subfolders \"Normal\" and \"Abnormal\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_tfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mval_ds\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_tfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mtest_ds\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_tfms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mallow_empty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     ):\n\u001b[0;32m--> 328\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[1;32m    147\u001b[0m     ) -> None:\n\u001b[1;32m    148\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         samples = self.make_dataset(\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/Train'"
          ]
        }
      ],
      "source": [
        "# ===== Imports =====\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms  # ImageFolder + transforms\n",
        "\n",
        "# ===== Config =====\n",
        "@dataclass\n",
        "class CFG:\n",
        "    # Paths\n",
        "    data_dir: str = \"./Data\"            # has Train, Validation, Test\n",
        "\n",
        "    # Training\n",
        "    epochs: int = 5\n",
        "    batch_size: int = 8\n",
        "    num_workers: int = 2\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 0.0\n",
        "    seed: int = 42\n",
        "\n",
        "    # Image/Transforms\n",
        "    img_size: int = 800\n",
        "    mean: float = 0.2636\n",
        "    std: float = 0.1562\n",
        "\n",
        "    # Checkpoints\n",
        "    save_dir: str = \"./checkpoints\"\n",
        "    ckpt_name: str = \"epoch=75-val_loss=0.69-val_acc=0.77.ckpt\"  # PRETRAINED (used for eval)\n",
        "    train_ckpt_name: str = \"best_train.pth\"                      # saved during 5-epoch training\n",
        "\n",
        "cfg = CFG()\n",
        "print(cfg)\n",
        "\n",
        "# ===== Transforms (TIFF supported via PIL under the hood) =====\n",
        "IM_SIZE = cfg.img_size\n",
        "MEAN = [cfg.mean, cfg.mean, cfg.mean]\n",
        "STD  = [cfg.std,  cfg.std,  cfg.std]\n",
        "\n",
        "# For DenseNet we provide 3-channel input; your data are grayscale TIFFs, so convert to 3 channels first.\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "# ===== Datasets & Loaders =====\n",
        "# Expect ImageFolder layout with /Train, /Validation, /Test under cfg.data_dir\n",
        "train_dir = os.path.join(cfg.data_dir, \"Train\")\n",
        "val_dir   = os.path.join(cfg.data_dir, \"Validation\")\n",
        "test_dir  = os.path.join(cfg.data_dir, \"Test\")\n",
        "\n",
        "# ImageFolder will read subfolders \"Normal\" and \"Abnormal\"\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform=val_tfms)\n",
        "test_ds  = datasets.ImageFolder(test_dir,  transform=val_tfms)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "print(f\"Classes: {class_names}\")\n",
        "\n",
        "pin = torch.cuda.is_available()\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=cfg.num_workers, pin_memory=pin)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=pin)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=pin)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Point CFG.data_dir to the detected DATA_DIR ===\n",
        "import os\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# 1) Locate Data folder robustly (you already saw it pick this path)\n",
        "CANDIDATES = [\n",
        "    Path(\"./Data\"),\n",
        "    Path(\"./classification/Data\"),\n",
        "    Path(\"./classification_repo/classification/Data\"),\n",
        "    Path(\"/content/classification_repo/classification/Data\"),\n",
        "]\n",
        "DATA_DIR = next((p.resolve() for p in CANDIDATES if p.exists()), None)\n",
        "if DATA_DIR is None:\n",
        "    raise FileNotFoundError(\"Couldn't find the Data folder. Checked: \" + \", \".join(map(str, CANDIDATES)))\n",
        "print(\"Using DATA_DIR:\", DATA_DIR)\n",
        "\n",
        "# 2) Config\n",
        "@dataclass\n",
        "class CFG:\n",
        "    # Paths\n",
        "    data_dir: str = str(DATA_DIR)       # <-- use the detected path\n",
        "    # Training\n",
        "    epochs: int = 5\n",
        "    batch_size: int = 8\n",
        "    num_workers: int = 2\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 0.0\n",
        "    seed: int = 42\n",
        "    # Image/Transforms\n",
        "    img_size: int = 800\n",
        "    mean: float = 0.2636\n",
        "    std: float = 0.1562\n",
        "    # Checkpoints\n",
        "    save_dir: str = \"./checkpoints\"\n",
        "    ckpt_name: str = \"epoch=75-val_loss=0.69-val_acc=0.77.ckpt\"\n",
        "    train_ckpt_name: str = \"best_train.pth\"\n",
        "\n",
        "cfg = CFG()\n",
        "print(cfg)\n",
        "\n",
        "# 3) Verify split folders exist\n",
        "for split in [\"Train\", \"Validation\", \"Test\"]:\n",
        "    sp = Path(cfg.data_dir) / split\n",
        "    print(f\"{split}: {sp}  exists: {sp.is_dir()}\")\n",
        "    if sp.is_dir():\n",
        "        # show immediate subfolders (class names) for sanity\n",
        "        try:\n",
        "            print(\"  classes:\", sorted([d.name for d in sp.iterdir() if d.is_dir()]))\n",
        "        except Exception as e:\n",
        "            print(\"  (could not list classes)\", e)\n",
        "\n",
        "# 4) Transforms\n",
        "IM_SIZE = cfg.img_size\n",
        "MEAN = [cfg.mean]*3\n",
        "STD  = [cfg.std]*3\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "# 5) Datasets & Loaders\n",
        "train_dir = os.path.join(cfg.data_dir, \"Train\")\n",
        "val_dir   = os.path.join(cfg.data_dir, \"Validation\")\n",
        "test_dir  = os.path.join(cfg.data_dir, \"Test\")\n",
        "\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform=val_tfms)\n",
        "test_ds  = datasets.ImageFolder(test_dir,  transform=val_tfms)\n",
        "\n",
        "print(\"Classes:\", train_ds.classes)\n",
        "\n",
        "pin = torch.cuda.is_available()\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=cfg.num_workers, pin_memory=pin)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=pin)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=pin)\n"
      ],
      "metadata": {
        "id": "4Fm5fxp3-jE6",
        "outputId": "c55d19bc-4d19-46d8-ea22-d1006149f25c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4Fm5fxp3-jE6",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using DATA_DIR: /content/classification_repo/classification/Data\n",
            "CFG(data_dir='/content/classification_repo/classification/Data', epochs=5, batch_size=8, num_workers=2, lr=0.0001, weight_decay=0.0, seed=42, img_size=800, mean=0.2636, std=0.1562, save_dir='./checkpoints', ckpt_name='epoch=75-val_loss=0.69-val_acc=0.77.ckpt', train_ckpt_name='best_train.pth')\n",
            "Train: /content/classification_repo/classification/Data/Train  exists: True\n",
            "  classes: ['Abnormal', 'Normal']\n",
            "Validation: /content/classification_repo/classification/Data/Validation  exists: True\n",
            "  classes: ['Abnormal', 'Normal']\n",
            "Test: /content/classification_repo/classification/Data/Test  exists: True\n",
            "  classes: ['Abnormal', 'Normal']\n",
            "Classes: ['Abnormal', 'Normal']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2abceb4",
      "metadata": {
        "id": "d2abceb4"
      },
      "source": [
        "## 4) Imports & Reproducibility\n",
        "from PIL import Image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b0c60285",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0c60285",
        "outputId": "0e1b662c-b0d7-4378-828e-9f257c36244b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import os, random, math, time\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets, models\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image  # <-- keep this on its own line, with no '\\n' before it\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(cfg.seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9097c9",
      "metadata": {
        "id": "7a9097c9"
      },
      "source": [
        "## 5) Datasets & Dataloaders\n",
        "We keep your normalization and effective grayscale handling. We convert grayscale to 3 channels so DenseNetâ€‘169 can ingest it, but the **statistics remain the same** (we just repeat the single channel values across RGB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "6a55d80c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a55d80c",
        "outputId": "e5b457fa-9959-4ccf-d583-b6fdfa05375d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['Abnormal', 'Normal'] -> num_classes=2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Transforms â€” training includes flips; validation only resize+normalize.\n",
        "# Convert grayscale TIFFs to 3 channels (RGB) so DenseNet works, but keep grayscale stats.\n",
        "\n",
        "IM_SIZE = cfg.img_size\n",
        "MEAN = [cfg.mean, cfg.mean, cfg.mean]\n",
        "STD  = [cfg.std,  cfg.std,  cfg.std]\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "# Expect ImageFolder layout with /Train and /Validation under cfg.data_dir\n",
        "train_dir = os.path.join(cfg.data_dir, \"Train\")\n",
        "val_dir   = os.path.join(cfg.data_dir, \"Validation\")\n",
        "\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform=val_tfms)\n",
        "\n",
        "class_names = train_ds.classes\n",
        "num_classes = len(class_names)\n",
        "print(f\"Classes: {class_names} -> num_classes={num_classes}\")\n",
        "\n",
        "pin = torch.cuda.is_available()\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=cfg.num_workers, pin_memory=pin)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=pin)\n",
        "\n",
        "len(train_ds), len(val_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3495666f",
      "metadata": {
        "id": "3495666f"
      },
      "source": [
        "## 6) Model â€” DenseNetâ€‘169 (pretrained)\n",
        "We replace the classifier with a 2â€‘unit linear layer for binary classification. Loss: `CrossEntropyLoss`. Optimizer: `Adam` (lr=1e-4 by default)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c413c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4c413c7",
        "outputId": "a5fa1ef4-45ee-4525-f1d3-747a7c8d7319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to /root/.cache/torch/hub/checkpoints/densenet169-b2777c0a.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 54.7M/54.7M [00:00<00:00, 211MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Build DenseNet-169\n",
        "dnet = models.densenet169(weights=models.DenseNet169_Weights.DEFAULT)\n",
        "\n",
        "# Replace classifier for our task\n",
        "in_features = dnet.classifier.in_features\n",
        "dnet.classifier = nn.Linear(in_features, 2)\n",
        "\n",
        "dnet = dnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(dnet.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544e6d31",
      "metadata": {
        "id": "544e6d31"
      },
      "source": [
        "## 7) Training Loop (10 epochs)\n",
        "A minimal, wellâ€‘commented loop with val accuracy each epoch. Checkpoints are saved to `cfg.save_dir`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "89382fc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "89382fc0",
        "outputId": "6709d42d-bf00-475e-8cb2-360707a80c18"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dnet' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2735547996.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dnet' is not defined"
          ]
        }
      ],
      "source": [
        "from typing import Dict\n",
        "import os, time\n",
        "import torch\n",
        "\n",
        "os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "best_val_acc = 0.0\n",
        "history: Dict[str, list] = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "def run_epoch(model: torch.nn.Module, loader: torch.utils.data.DataLoader, train: bool) -> tuple[float, float]:\n",
        "    epoch_loss = 0.0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "\n",
        "    if train:\n",
        "        model.train()\n",
        "        ctx = torch.enable_grad()\n",
        "    else:\n",
        "        model.eval()\n",
        "        ctx = torch.inference_mode()\n",
        "\n",
        "    with ctx:\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)              # logits\n",
        "            loss = criterion(outputs, labels)    # CE loss\n",
        "\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # accuracy on the fly\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / max(1, total)\n",
        "    acc = correct / max(1, total)\n",
        "    return avg_loss, acc\n",
        "\n",
        "for epoch in range(1, cfg.epochs + 1):\n",
        "    t0 = time.time()\n",
        "    train_loss, _      = run_epoch(dnet, train_loader, train=True)\n",
        "    val_loss, val_acc  = run_epoch(dnet, val_loader,  train=False)\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_path = os.path.join(cfg.save_dir, cfg.train_ckpt_name)\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model_state\": dnet.state_dict(),\n",
        "                \"class_names\": class_names,\n",
        "                \"cfg\": vars(cfg),\n",
        "            },\n",
        "            best_path,\n",
        "        )\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d}/{cfg.epochs} | \"\n",
        "        f\"train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | val_acc: {val_acc:.4f} | \"\n",
        "        f\"time: {t1 - t0:.1f}s\"\n",
        "    )\n",
        "\n",
        "print(f\"Best val_acc: {best_val_acc:.4f} | Saved to: {os.path.join(cfg.save_dir, cfg.train_ckpt_name)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================== ONE-CELL TRAINING PIPELINE ===================\n",
        "# Safe to run from a fresh Colab runtime.\n",
        "import os, random, time\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "# --- Reproducibility ---\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --- Locate Data folder robustly ---\n",
        "CANDIDATES = [\n",
        "    Path(\"./Data\"),\n",
        "    Path(\"./classification/Data\"),\n",
        "    Path(\"./classification_repo/classification/Data\"),\n",
        "    Path(\"/content/classification_repo/classification/Data\"),\n",
        "]\n",
        "DATA_DIR = next((p.resolve() for p in CANDIDATES if p.exists()), None)\n",
        "if DATA_DIR is None:\n",
        "    raise FileNotFoundError(\"Couldn't find Data folder. Checked:\\n\" + \"\\n\".join(map(str, CANDIDATES)))\n",
        "print(\"Using DATA_DIR:\", DATA_DIR)\n",
        "\n",
        "# --- Config ---\n",
        "@dataclass\n",
        "class CFG:\n",
        "    # Paths\n",
        "    data_dir: str = str(DATA_DIR)\n",
        "    save_dir: str = \"./checkpoints\"\n",
        "    train_ckpt_name: str = \"best_train.pth\"\n",
        "    # Training\n",
        "    epochs: int = 5\n",
        "    batch_size: int = 4\n",
        "    num_workers: int = 0\n",
        "    lr: float = 1e-4\n",
        "    weight_decay: float = 0.0\n",
        "    seed: int = 42\n",
        "    # Image/Transforms\n",
        "    img_size: int = 224\n",
        "    mean: float = 0.2636\n",
        "    std: float = 0.1562\n",
        "\n",
        "cfg = CFG()\n",
        "os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "print(cfg)\n",
        "\n",
        "# --- Verify split folders ---\n",
        "for split in [\"Train\", \"Validation\"]:\n",
        "    sp = Path(cfg.data_dir) / split\n",
        "    print(f\"{split}: {sp} exists -> {sp.is_dir()}\")\n",
        "\n",
        "# --- Transforms ---\n",
        "IM_SIZE = cfg.img_size\n",
        "MEAN = [cfg.mean]*3\n",
        "STD  = [cfg.std]*3\n",
        "\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "val_tfms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((IM_SIZE, IM_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "# --- Datasets & Loaders ---\n",
        "train_dir = os.path.join(cfg.data_dir, \"Train\")\n",
        "val_dir   = os.path.join(cfg.data_dir, \"Validation\")\n",
        "\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=train_tfms)\n",
        "val_ds   = datasets.ImageFolder(val_dir,   transform=val_tfms)\n",
        "class_names = train_ds.classes\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes:\", class_names, \"| num_classes:\", num_classes)\n",
        "\n",
        "pin = torch.cuda.is_available()\n",
        "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
        "                          num_workers=cfg.num_workers, pin_memory=pin)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=cfg.batch_size, shuffle=False,\n",
        "                          num_workers=cfg.num_workers, pin_memory=pin)\n",
        "\n",
        "# --- Model / Loss / Optimizer ---\n",
        "try:\n",
        "    dnet = models.densenet169(weights=models.DenseNet169_Weights.DEFAULT)\n",
        "except Exception:\n",
        "    # Fallback if weights can't be downloaded\n",
        "    dnet = models.densenet169(weights=None)\n",
        "\n",
        "in_features = dnet.classifier.in_features\n",
        "dnet.classifier = nn.Linear(in_features, num_classes)\n",
        "dnet = dnet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(dnet.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
        "\n",
        "# --- Train/Eval helpers ---\n",
        "def run_epoch(model: torch.nn.Module, loader: torch.utils.data.DataLoader, train: bool) -> tuple[float, float]:\n",
        "    epoch_loss, total, correct = 0.0, 0, 0\n",
        "    model.train(mode=train)\n",
        "    ctx = torch.enable_grad() if train else torch.inference_mode()\n",
        "    with ctx:\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            if train:\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item() * images.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = epoch_loss / max(1, total)\n",
        "    acc = correct / max(1, total)\n",
        "    return avg_loss, acc\n",
        "\n",
        "# --- Train loop ---\n",
        "best_val_acc = 0.0\n",
        "history: Dict[str, list] = {\"train_loss\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "for epoch in range(1, cfg.epochs + 1):\n",
        "    t0 = time.time()\n",
        "    train_loss, _     = run_epoch(dnet, train_loader, train=True)\n",
        "    val_loss, val_acc = run_epoch(dnet, val_loader,  train=False)\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        best_path = os.path.join(cfg.save_dir, cfg.train_ckpt_name)\n",
        "        torch.save(\n",
        "            {\"model_state\": dnet.state_dict(),\n",
        "             \"class_names\": class_names,\n",
        "             \"cfg\": vars(cfg)},\n",
        "            best_path,\n",
        "        )\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d}/{cfg.epochs} | \"\n",
        "        f\"train_loss: {train_loss:.4f} | val_loss: {val_loss:.4f} | val_acc: {val_acc:.4f} | \"\n",
        "        f\"time: {t1 - t0:.1f}s\"\n",
        "    )\n",
        "\n",
        "print(f\"Best val_acc: {best_val_acc:.4f} | Saved: {os.path.join(cfg.save_dir, cfg.train_ckpt_name)}\")\n",
        "# =================================================================\n"
      ],
      "metadata": {
        "id": "xMWn0a3F_NWZ",
        "outputId": "f9a0291a-9f0a-4c22-ff30-99dd3a9901e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "xMWn0a3F_NWZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Using DATA_DIR: /content/classification_repo/classification/Data\n",
            "CFG(data_dir='/content/classification_repo/classification/Data', save_dir='./checkpoints', train_ckpt_name='best_train.pth', epochs=5, batch_size=4, num_workers=0, lr=0.0001, weight_decay=0.0, seed=42, img_size=800, mean=0.2636, std=0.1562)\n",
            "Train: /content/classification_repo/classification/Data/Train exists -> True\n",
            "Validation: /content/classification_repo/classification/Data/Validation exists -> True\n",
            "Classes: ['Abnormal', 'Normal'] | num_classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb45af23",
      "metadata": {
        "id": "fb45af23"
      },
      "source": [
        "## 8) Plot Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bca546a",
      "metadata": {
        "id": "7bca546a"
      },
      "outputs": [],
      "source": [
        "# Plot loss and val accuracy\n",
        "plt.figure()\n",
        "plt.plot(history[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"loss\"); plt.legend(); plt.title(\"Loss\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history[\"val_acc\"], label=\"val_acc\")\n",
        "plt.xlabel(\"epoch\"); plt.ylabel(\"accuracy\"); plt.legend(); plt.title(\"Validation Accuracy\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ccdfb8e",
      "metadata": {
        "id": "6ccdfb8e"
      },
      "source": [
        "## 9) Evaluation â€” Load Pretrained Weights and Test\n",
        "Point `weights_path` to your pretrained checkpoint (`.pth`) saved by this notebook **or your own weights**. We provide both **singleâ€‘image test** and **folderâ€‘based batch test** with a confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c4d95a",
      "metadata": {
        "id": "70c4d95a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def _strip_prefix_if_present(state_dict, prefixes=(\"model.\", \"net.\", \"module.\", \"backbone.\")):\n",
        "    new_sd = {}\n",
        "    for k, v in state_dict.items():\n",
        "        new_key = k\n",
        "        for p in prefixes:\n",
        "            if new_key.startswith(p):\n",
        "                new_key = new_key[len(p):]\n",
        "        new_sd[new_key] = v\n",
        "    return new_sd\n",
        "\n",
        "def load_model_for_inference(weights_path: str, num_classes=2, class_names_override=None):\n",
        "    model = models.densenet169(weights=models.DenseNet169_Weights.DEFAULT)\n",
        "    in_features = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(in_features, num_classes)\n",
        "    ckpt = torch.load(weights_path, map_location=\"cpu\")\n",
        "    if isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
        "        state_dict = ckpt[\"state_dict\"]\n",
        "    elif isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
        "        state_dict = ckpt[\"model_state\"]\n",
        "    else:\n",
        "        state_dict = ckpt\n",
        "    state_dict = _strip_prefix_if_present(state_dict)\n",
        "    model.load_state_dict(state_dict, strict=False)\n",
        "    model.eval().to(device)\n",
        "    if isinstance(ckpt, dict):\n",
        "        class_names = ckpt.get(\"class_names\", class_names_override or [\"Normal\", \"Abnormal\"])\n",
        "    else:\n",
        "        class_names = class_names_override or [\"Normal\", \"Abnormal\"]\n",
        "    return model, class_names\n",
        "\n",
        "print(\"Inference loader ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86d745cd",
      "metadata": {
        "id": "86d745cd"
      },
      "source": [
        "### 9.a) Example: load weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5129136",
      "metadata": {
        "id": "f5129136"
      },
      "outputs": [],
      "source": [
        "# Evaluation will use the PRETRAINED checkpoint (do not overwrite).\n",
        "weights_path = os.path.join(cfg.save_dir, cfg.ckpt_name)\n",
        "model_inf, class_names_inf = load_model_for_inference(weights_path)\n",
        "print(\"Loaded inference model; classes:\", class_names_inf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4792871",
      "metadata": {
        "id": "f4792871"
      },
      "source": [
        "### 9.b) Singleâ€‘image prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f779d1",
      "metadata": {
        "id": "84f779d1"
      },
      "outputs": [],
      "source": [
        "# Set an example image\n",
        "# img_path = \"/content/data/val/Normal/example_001.png\"\n",
        "# result = predict_single_image(img_path, model_inf, class_names_inf)\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c9f2848",
      "metadata": {
        "id": "4c9f2848"
      },
      "source": [
        "### 9.c) Batch evaluation on a folder (e.g., your validation set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb12f612",
      "metadata": {
        "id": "cb12f612"
      },
      "outputs": [],
      "source": [
        "# data_root = \"/content/data/val\"\n",
        "# cm, report, classes = evaluate_folder(data_root, model_inf, val_tfms)\n",
        "# print(\"Classes:\", classes)\n",
        "# print(\"Classification report:\\n\", report)\n",
        "\n",
        "# # Plot confusion matrix (no seaborn, simple matplotlib)\n",
        "# import itertools\n",
        "# plt.figure()\n",
        "# plt.imshow(cm, interpolation='nearest')\n",
        "# plt.title(\"Confusion Matrix\")\n",
        "# plt.colorbar()\n",
        "# tick_marks = np.arange(len(classes))\n",
        "# plt.xticks(tick_marks, classes, rotation=45)\n",
        "# plt.yticks(tick_marks, classes)\n",
        "\n",
        "# thresh = cm.max() / 2.\n",
        "# for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "#     plt.text(j, i, format(cm[i, j], 'd'),\n",
        "#              horizontalalignment=\"center\",\n",
        "#              color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "# plt.ylabel('True label')\n",
        "# plt.xlabel('Predicted label')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c14a34d4",
      "metadata": {
        "id": "c14a34d4"
      },
      "source": [
        "## 10) Notes\n",
        "- Image size kept at **800Ã—800** to match your code.\n",
        "- Normalization kept at **mean=0.2636**, **std=0.1562** (replicated across 3 channels under the hood).\n",
        "- Random H/V flips used in training, none in validation â€” consistent with your approach.\n",
        "- You can easily swap the dataset paths or use Drive.\n",
        "- For a quick run, keep 10 epochs; adjust later as needed.\n",
        "- If your labels are reversed (0/1 mapping), just read `class_names` from `train_ds.classes` and interpret accordingly.\n",
        "- The evaluation helpers mimic your testing flow: softmax probs, perâ€‘image prediction, and a batch confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4b255df",
      "metadata": {
        "id": "f4b255df"
      },
      "outputs": [],
      "source": [
        "# === Fetch PRETRAINED weights from Google Drive (only this file) ===\n",
        "# Requires link access set to 'Anyone with the link' in Google Drive.\n",
        "try:\n",
        "    import gdown  # type: ignore\n",
        "except ImportError:\n",
        "    import sys, subprocess\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"gdown\"])\n",
        "    import gdown  # type: ignore\n",
        "\n",
        "import os\n",
        "os.makedirs(cfg.save_dir, exist_ok=True)\n",
        "\n",
        "drive_file_id = \"1HoiZ0waLFLc4Yse9-ITS0sKOhC8KhT3n\"\n",
        "dest_path = os.path.join(cfg.save_dir, cfg.ckpt_name)\n",
        "\n",
        "if not os.path.exists(dest_path):\n",
        "    url = f\"https://drive.google.com/uc?id={drive_file_id}\"\n",
        "    print(\"Downloading pretrained weights to:\", dest_path)\n",
        "    gdown.download(url, dest_path, quiet=False)\n",
        "else:\n",
        "    print(\"Pretrained weights already present at:\", dest_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d40502d",
      "metadata": {
        "id": "2d40502d"
      },
      "outputs": [],
      "source": [
        "# === Evaluate PRETRAINED weights on TEST ===\n",
        "weights_path = os.path.join(cfg.save_dir, cfg.ckpt_name)\n",
        "model_inf, class_names_inf = load_model_for_inference(weights_path, num_classes=len(class_names))\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        logits = model_inf(images)\n",
        "        preds = logits.argmax(dim=1).cpu().numpy().tolist()\n",
        "        y_pred.extend(preds)\n",
        "        y_true.extend(labels.numpy().tolist())\n",
        "\n",
        "print(\"Classes:\", class_names_inf)\n",
        "print(classification_report(y_true, y_pred, target_names=test_ds.classes, digits=4))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}